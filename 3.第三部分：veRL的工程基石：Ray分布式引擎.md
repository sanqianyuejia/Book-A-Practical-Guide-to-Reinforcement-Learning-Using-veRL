# 第三部分：veRL 的工程基石：Ray 分布式引擎
## 1. Ray 基础概念与核心功能
Ray 是一个用于构建分布式应用程序的开源框架，它提供了一个统一的 API 来并行化 Python 和 Java 代码。作为 veRL 的工程基石，Ray 为其提供了强大的分布式计算能力，特别适用于需要大规模并行处理的机器学习工作负载，包括强化学习。Ray 的核心优势在于其动态计算图和异构调度能力，这使得它能够高效地处理复杂的机器人学习任务。

~~~
参考文献：
1. Building LLM Training Systems at Scale: https://mlsyscourse.org/slides/HaibinLinTalk.pdf
2. OpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework: https://arxiv.org/html/2405.11143v1
~~~

Ray 的核心功能包括：
 - **任务（Tasks）**：Ray 任务是用于无状态计算的基本单元。用户可以将普通的 Python 函数作为 Ray 任务远程执行，Ray 会自动处理任务的调度和执行。
 - **执行者（Actors）**：<span style="color: red;">Ray 执行者是用于有状态计算的基本单元。</span>用户可以定义 Python 类作为 Ray 执行者，每个执行者都维护着自己的状态，并且可以被并发地调用。<span style="color: red;">在 veRL 中，例如 Actor、Critic 和 Reward 模型通常被实现为 Ray 的执行者。</span>
 - ***分布式对象存储（Distributed Object Store）**：Ray 提供了一个分布式的内存对象存储系统，允许任务和执行者之间高效地共享数据。这对于机器学习应用非常重要，因为模型参数和训练数据通常需要在不同的计算节点之间频繁地传递。<span style="color: red;">这个与 HybridFlow 中的 Dataflow 有什么关联吗？</span>
 - **调度（Scheduling）**：Ray 具有先进的调度能力，能够根据资源可用性和任务依赖关系，智能地将任务和执行者分配到集群中的不同节点上执行。Ray 的调度器能够处理异构的计算资源，并支持动态的任务调度。

Ray 之所以非常适合分布式强化学习，是因为它能够很好地支持并行性、可扩展性和容错性。强化学习训练通常需要大量的环境交互和模型更新，这些过程可以通过 Ray 的任务和执行者进行并行化处理。Ray 的分布式架构可以轻松地扩展到数千个节点，从而支持训练非常大的模型。此外，Ray 提供了内置的容错机制，能够在节点发生故障时自动恢复任务，保证了训练过程的稳定性。

总而言之，Ray 作为一个强大的分布式计算框架，为 veRL 提供了必要的底层支持，其核心功能包括任务、执行者、分布式对象存储和灵活的调度，这些功能对于实现强化学习的并行化、可扩展性和容错性至关重要。

~~~
参考文献：
1. OpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework: https://arxiv.org/html/2405.11143v1
~~~

## 2. veRL 如何利用 Ray 实现分布式训练
veRL 在架构层面与 Ray 进行了深度集成，通过自定义的 Ray 训练器（Trainers）和执行者（Actors）来实现分布式训练。<span style="color: red;">这里需要仔细澄清？因为我在verl源代码中未找到Actors的痕迹</span>在 veRL 中，强化学习的控制器（Controller）通常被实现为 Ray 的远程函数，负责协调整个训练过程。而不同的工作者组（Worker Groups），例如用于生成经验数据的 Actor 工作者、用于评估价值函数的 Critic 工作者以及用于计算奖励的 Reward 工作者，则被实现为 Ray 的执行者。这些执行者运行在集群的不同节点上，实现了计算的分布式执行。

~~~
这里对于 Ray 被 veRL 使用，存在较大的疑问？
veRL 中，有哪些事情需要借助 Ray 来完成？
1. Controller 控制器？实现 control flow 的传递？
2. 任务分发？实现任务的调度？
3. 数据共享？实现 data flow 的传递？
更多？？？
~~~

Ray 在 veRL 中主要通过以下方式实现分布式数据并行：控制器负责将训练数据分割成多个部分，并将这些数据分发给不同的工作组中的工作者进行并行处理。每个工作者在其分配到的数据上执行计算任务（例如，生成经验数据或计算梯度），并将结果返回给控制器进行聚合和后续处理（例如，模型参数更新）。

对于大规模模型的分布式模型并行，veRL 结合了 Ray 与 FSDP 和 Megatron-LM 。<span style="color: red;">FSDP 难道不是负责数据并行吗？怎么被 Ray 给抢走了？</span>虽然 FSDP 和 Megatron-LM 负责模型的分片和在多个 GPU 上的分布，但 Ray 在 veRL 中扮演着编排的角色，管理着涉及到这些模型并行组件的整个分布式训练过程。<span style="color: red;">Megatron-LM 负责模型并行，FSDP 负责数据并行，Ray 与这两者之间的协作关系是什么？？？？</span> Ray 确保了不同并行模式下的数据和计算能够正确地协调和同步。

~~~
这里存在大量的疑问：
1. Ray 与 FSDP 和 Megatron-LM 之间的协作关系是什么？
2. 到底谁负责数据并行？？？谁负责模型并行？？？
3. Ray 在 veRL 中，到底扮演着什么样的角色？？？
4. 工作者 Worker 在 veRL 中，处在什么层次？WorkerGroup 又处在什么层次？？？

搞蒙了，需要重新梳理，看代码、看论文？？？？
~~~

总而言之，veRL 利用 Ray 通过将控制器定义为远程函数，将工作者定义为执行者来实现分布式训练。<span style="color: red;">这样的结论，应该在上文中尽早给出来，不然读者看着一脸蒙圈</span> Ray 的数据并行能力使得训练数据可以在多个工作者之间进行分配，<span style="color: red;">为什么反复强调 Ray 具备数据并行能力？？？Ray 不是一个分布式任务计算框架吗？？？怎么又具备了数据并行能力了？？？</span>而 Ray 又与 FSDP 和 Megatron-LM 协同工作，实现了对大语言模型的分布式模型并行。

~~~
疑问：
1. 这里提到的 veRL 中工作者（Worker，比如RewardModelWorker，ActorRolloutRefWorker等），与 Ray 中的概念术语 执行者（Actor），是对应上的？？？
2. 为了避免将强化学习中的概念 Actor（演员模型）与 Ray 中的 Actor（执行者）混淆，所以，在 veRL 中，直接将 执行者以 Worker 命名？？？？又或者，Ray 中的执行者，本应翻译为 Worker？？？
~~~

## 3. 使用 Ray 进行任务并行与资源管理
在 veRL 中，Ray 的任务（Tasks）被广泛应用于并行执行强化学习训练流程的不同部分，例如经验数据（Rollout）的生成和梯度计算。veRL 中工作者的方法通常被定义为 Ray 的远程函数，这些函数可以在集群中的多个节点上并行执行，从而加速训练过程。例如，生成经验数据的函数可以在多个 Actor 工作者上并行运行，从而快速收集大量的训练样本。

~~~
这里关于 Ray 的任务并行解释的很含糊，看完不知所以然？？？？
是否可以通过解析 veRL 中的代码架构，来深入阐述 Ray 的任务并行的用法？？？
任务并行的痛点有哪些？Ray 是如何解决这些痛点的？
并通过阅读 Ray 的源码，以便进一步解释 Ray 中的任务并行机制？？？
以及 Ray 为了实现任务并行功能，所使用的相关技术？？？
~~~

Ray 的资源管理功能使得 veRL 能够高效地利用分布式集群中的 GPU 资源。用户可以在定义 Ray 的任务和执行者时指定其所需的资源（例如，GPU 的数量），Ray 的调度器会根据集群中资源的可用情况，将任务和执行者分配到合适的节点上执行。<span style="color: red;">此处，应该对**节点**的概念进行解释</span> 在 veRL 中，可以为不同的工作者组指定不同的资源需求，例如，Actor 工作者可能需要更多的 GPU 来进行模型推理和经验生成，而 Critic 工作者可能需要较少的 GPU 来进行价值评估。Ray 的资源管理确保了资源能够被合理地分配和利用，避免了资源竞争和浪费。

~~~
资源的管理方面，有哪些痛点？
Ray 是如何解决这些痛点的？
~~~

Ray 的动态调度器在优化 veRL 训练任务的执行方面起着至关重要的作用。调度器能够根据集群中资源的实时可用情况以及任务之间的依赖关系，动态地调整任务的执行顺序和位置，从而最大限度地提高训练效率。这种动态调度能力使得 veRL 能够很好地适应异构的计算环境和变化的负载情况。

~~~
1. 动态调度器解决的是什么痛点？
2. 异构环境给训练带来了什么挑战？
3. Ray 的动态调度器是如何解决这种挑战的？
4. 集群资源是如何被 Ray 最大化的利用？
5. 任务之间的依赖关系为什么给调度器增加了挑战？
~~~

总而言之，veRL 通过利用 Ray 的任务并行能力，实现了训练流程中关键步骤的并行执行，并通过 Ray 的资源管理功能，高效地分配和调度 GPU 资源，从而优化了整个强化学习的训练过程。

## 4. 在 veRL 中部署和管理 Ray 集群
为了配合 veRL 使用，用户需要部署和管理 Ray 集群。Ray 提供了多种部署选项，包括在本地机器上搭建单节点或多节点集群，以及在云平台上部署大规模集群。<span style="color: red;">这里如果不来个示例，读者就根本看不明白，这个章节也就没有任何价值。</span> veRL 的文档通常会提供关于如何设置 Ray 集群的详细说明，包括安装 Ray 的步骤、启动集群的命令以及配置集群参数的方法。

~~~
1. 提供一个对集群直观的描述，动手教读者怎么做？
2. 单节点集群如何创建？
3. 多节点集群如何创建？
4. 如何在云平台上部署大规模集群？
5. 包安装、启动以及配置的方法，一步步说出来
~~~

管理一个运行中的 Ray 集群对于使用 veRL 进行长时间的强化学习训练至关重要。Ray 提供了一系列工具和接口，用于监控集群的资源利用率（例如，CPU、GPU、内存的使用情况），查看运行中的任务和执行者的状态，以及进行集群的伸缩（例如，添加或删除节点）。用户可以利用这些工具来了解训练的进展情况，及时发现和解决潜在的性能瓶颈或故障。例如，如果发现某些节点的 GPU 利用吕过高或过低，可以考虑调整任务的资源需求或集群的规模。

~~~
1. Ray 提供的工具和接口，有哪些？使用列表罗列出来
2. 如何进行集群的伸缩？
3. 如何解决性能瓶颈？
4. 如何解决故障？
~~~

如果用户以及拥有一个正在运行的 Ray 集群，veRL 允许直接将训练任务提交到该集群上运行。<span style="color: red;">为什么可以？很魔幻，如果能解释清楚，那么能解用户心中之大疑惑</span> 这为那些已经构建了 Ray 基础设施的用户提供了便利，<span style="color: red;"> 为什么这么说？？难道 veRL 可以与 Ray 集群做代码隔离吗？？？</span> 他们可以轻松地将 veRL 集成到现有的工作流程中。用户只需要配置 veRL 连接到 Ray 集群的地址和端口，就可以开始在集群上运行强化学习训练任务。

~~~
按照这一段最后一句说法，veRL 与 Ray 集群的集成，是有两种方式的，即：
1. veRL 的进程中来创建 Ray 集群？？？
2. veRL 通过网络连接来访问独立的 Ray 集群？？？

这是很酷的设计方法，将基础设施与开源平台解耦，以便灵活地适配当前环境。
~~~

总而言之，在 veRL 中部署和管理 Ray 集群，包括根据需求设置集群（本地或云端），利用 Ray 提供的工具监控资源使用情况和任务状态，根据需要伸缩集群，以及将 veRL 配置为连接到现有的 Ray 集群。

~~~
1. 实操性：有核心模块的解释，有示例，有结果
2. 照着示例动手就能掌握
3. 足够多的疑问，并针对这些疑问，有足够清晰的说明或解决方案？？？？
~~~