# 第三部分：veRL 的工程基石：Ray 分布式引擎
## 1. Ray 基础概念与核心功能
Ray 是一个用于构建分布式应用程序的开源框架，它提供了一个统一的 API 来并行化 Python 和 Java 代码。作为 veRL 的工程基石，Ray 为其提供了强大的分布式计算能力，特别适用于需要大规模并行处理的机器学习工作负载，包括强化学习。Ray 的核心优势在于其动态计算图和异构调度能力，这使得它能够高效地处理复杂的机器人学习任务。

~~~
参考文献：
1. Building LLM Training Systems at Scale: https://mlsyscourse.org/slides/HaibinLinTalk.pdf
2. OpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework: https://arxiv.org/html/2405.11143v1
~~~

Ray 的核心功能包括：
 - **任务（Tasks）**：Ray 任务是用于无状态计算的基本单元。用户可以将普通的 Python 函数作为 Ray 任务远程执行，Ray 会自动处理任务的调度和执行。
 - **执行者（Actors）**：<span style="color: red;">Ray 执行者是用于有状态计算的基本单元。</span>用户可以定义 Python 类作为 Ray 执行者，每个执行者都维护着自己的状态，并且可以被并发地调用。<span style="color: red;">在 veRL 中，例如 Actor、Critic 和 Reward 模型通常被实现为 Ray 的执行者。</span>
 - ***分布式对象存储（Distributed Object Store）**：Ray 提供了一个分布式的内存对象存储系统，允许任务和执行者之间高效地共享数据。这对于机器学习应用非常重要，因为模型参数和训练数据通常需要在不同的计算节点之间频繁地传递。<span style="color: red;">这个与 HybridFlow 中的 Dataflow 有什么关联吗？</span>
 - **调度（Scheduling）**：Ray 具有先进的调度能力，能够根据资源可用性和任务依赖关系，智能地将任务和执行者分配到集群中的不同节点上执行。Ray 的调度器能够处理异构的计算资源，并支持动态的任务调度。

Ray 之所以非常适合分布式强化学习，是因为它能够很好地支持并行性、可扩展性和容错性。强化学习训练通常需要大量的环境交互和模型更新，这些过程可以通过 Ray 的任务和执行者进行并行化处理。Ray 的分布式架构可以轻松地扩展到数千个节点，从而支持训练非常大的模型。此外，Ray 提供了内置的容错机制，能够在节点发生故障时自动恢复任务，保证了训练过程的稳定性。

总而言之，Ray 作为一个强大的分布式计算框架，为 veRL 提供了必要的底层支持，其核心功能包括任务、执行者、分布式对象存储和灵活的调度，这些功能对于实现强化学习的并行化、可扩展性和容错性至关重要。

~~~
参考文献：
1. OpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework: https://arxiv.org/html/2405.11143v1
~~~

## 2. veRL 如何利用 Ray 实现分布式训练
veRL 在架构层面与 Ray 进行了深度集成，通过自定义的 Ray 训练器（Trainers）和执行者（Actors）来实现分布式训练。<span style="color: red;">这里需要仔细澄清？因为我在verl源代码中未找到Actors的痕迹</span>在 veRL 中，强化学习的控制器（Controller）通常被实现为 Ray 的远程函数，负责协调整个训练过程。而不同的工作者组（Worker Groups），例如用于生成经验数据的 Actor 工作者、用于评估价值函数的 Critic 工作者以及用于计算奖励的 Reward 工作者，则被实现为 Ray 的执行者。这些执行者运行在集群的不同节点上，实现了计算的分布式执行。

~~~
这里对于 Ray 被 veRL 使用，存在较大的疑问？
veRL 中，有哪些事情需要借助 Ray 来完成？
1. Controller 控制器？实现 control flow 的传递？
2. 任务分发？实现任务的调度？
3. 数据共享？实现 data flow 的传递？
更多？？？
~~~

Ray 在 veRL 中主要通过以下方式实现分布式数据并行：控制器负责将训练数据分割成多个部分，并将这些数据分发给不同的工作组中的工作者进行并行处理。每个工作者在其分配到的数据上执行计算任务（例如，生成经验数据或计算梯度），并将结果返回给控制器进行聚合和后续处理（例如，模型参数更新）。

对于大规模模型的分布式模型并行，veRL 结合了 Ray 与 FSDP 和 Megatron-LM 。<span style="color: red;">FSDP 难道不是负责数据并行吗？怎么被 Ray 给抢走了？</span>虽然 FSDP 和 Megatron-LM 负责模型的分片和在多个 GPU 上的分布，但 Ray 在 veRL 中扮演着编排的角色，管理着涉及到这些模型并行组件的整个分布式训练过程。<span style="color: red;">Megatron-LM 负责模型并行，FSDP 负责数据并行，Ray 与这两者之间的协作关系是什么？？？？</span> Ray 确保了不同并行模式下的数据和计算能够正确地协调和同步。

~~~
这里存在大量的疑问：
1. Ray 与 FSDP 和 Megatron-LM 之间的协作关系是什么？
2. 到底谁负责数据并行？？？谁负责模型并行？？？
3. Ray 在 veRL 中，到底扮演着什么样的角色？？？
4. 工作者 Worker 在 veRL 中，处在什么层次？WorkerGroup 又处在什么层次？？？

搞蒙了，需要重新梳理，看代码、看论文？？？？
~~~

总而言之，veRL 利用 Ray 通过将控制器定义为远程函数，将工作者定义为执行者来实现分布式训练。<span style="color: red;">这样的结论，应该在上文中尽早给出来，不然读者看着一脸蒙圈</span> Ray 的数据并行能力使得训练数据可以在多个工作者之间进行分配，<span style="color: red;">为什么反复强调 Ray 具备数据并行能力？？？Ray 不是一个分布式任务计算框架吗？？？怎么又具备了数据并行能力了？？？</span>而 Ray 又与 FSDP 和 Megatron-LM 协同工作，实现了对大语言模型的分布式模型并行。

~~~
疑问：
1. 这里提到的 veRL 中工作者（Worker，比如RewardModelWorker，ActorRolloutRefWorker等），与 Ray 中的概念术语 执行者（Actor），是对应上的？？？
2. 为了避免将强化学习中的概念 Actor（演员模型）与 Ray 中的 Actor（执行者）混淆，所以，在 veRL 中，直接将 执行者以 Worker 命名？？？？又或者，Ray 中的执行者，本应翻译为 Worker？？？
~~~

## 3. 使用 Ray 进行任务并行与资源管理
在 veRL 中，Ray 的任务（Tasks）被广泛应用于并行执行强化学习训练流程的不同部分，例如经验数据（Rollout）的生成和梯度计算。veRL 中工作者的方法通常被定义为 Ray 的远程函数，这些函数可以在集群中的多个节点上并行执行，从而加速训练过程。例如，生成经验数据的函数可以在多个 Actor 工作者上并行运行，从而快速收集大量的训练样本。

~~~
这里关于 Ray 的任务并行解释的很含糊，看完不知所以然？？？？
是否可以通过解析 veRL 中的代码架构，来深入阐述 Ray 的任务并行的用法？？？
并通过阅读 Ray 的源码，以便进一步解释 Ray 中的任务并行机制？？？
以及 Ray 为了实现任务并行功能，所使用的相关技术？？？
~~~

Ray 的资源管理功能使得 veRL 能够高效地利用分布式集群中的 GPU 资源。用户可以在定义 Ray 的任务和执行者时指定其所需的资源（例如，GPU 的数量），Ray 的调度器会根据集群中资源的可用情况，将任务和执行者分配到合适的节点上执行。<span style="color: red;">此处，应该对**节点**的概念进行解释</span> 在 veRL 中，可以为不同的工作者组指定不同的资源需求，例如，Actor 工作者可能需要更多的 GPU 来进行模型推理和经验生成，而 Critic 工作者可能需要较少的 GPU 来进行价值评估。Ray 的资源管理确保了资源能够被合理地分配和利用，避免了资源竞争和浪费。

~~~
资源的管理，

## 4. 在 veRL 中部署和管理 Ray 集群